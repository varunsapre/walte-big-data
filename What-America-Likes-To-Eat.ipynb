{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i1sSWOUeEi9"
      },
      "source": [
        "# Intialization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install pyspark"
      ],
      "metadata": {
        "id": "kLMOLn5AVUct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2R63X2G2nZJ",
        "outputId": "1596d8bd-b473-4c0e-d153-d54ce3770aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOqTBDjt8kbT"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r4nOta7eWRM"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import logging as l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZFquOss8moL"
      },
      "source": [
        "Mount google drive to share data across collaboraters. Initialize data source from google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfMsq0BuhCjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e146e4-9b2a-417a-d49a-41959f2c5a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/data; to attempt to forcibly remount, call drive.mount(\"/content/data\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvmceGuTfOeL"
      },
      "outputs": [],
      "source": [
        "l.basicConfig(format='%(levelname)s | %(asctime)s | %(message)s', level=l.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uiIYTLMfUKY"
      },
      "outputs": [],
      "source": [
        "FILESYSTEM = \"file://\" # localfile needs 'file://' else \"hdfs://\"\n",
        "HDFS_NODE = \"\" # hostname and namenode port, default = 9000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDiqapsT8scU"
      },
      "source": [
        "Init spark session and context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POyiEwNJfevy",
        "outputId": "212bdd24-6f2d-4894-fc77-2d4b63fa13d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO | 2021-12-10 23:29:55,212 | -----SPARK SESSION STARTED-----\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName('local[*]').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"WARN\")\n",
        "l.info(\"-----SPARK SESSION STARTED-----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6nDPdLNkhAD"
      },
      "source": [
        "# Data Loading and Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Yelp business json file that contains details of 160,585 businesses collected across several cities"
      ],
      "metadata": {
        "id": "iMyT7QlHWEj9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ddv1k-S8vxV"
      },
      "source": [
        "Read from hdfs/local data file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB4TmKJCe2KJ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "HDFS_PATH= \"/content/data/MyDrive/bdm-data/Yelp/yelp_academic_dataset_business.json\"\n",
        "# if not hdfs, disregard node variable\n",
        "if \"hdfs\" not in FILESYSTEM:\n",
        "    HDFS_NODE = \"\"\n",
        "\n",
        "FILE_ACCESS = f\"{FILESYSTEM}{HDFS_NODE}{HDFS_PATH}\"\n",
        "\n",
        "l.info(f\"Accesing file: {FILE_ACCESS}\")\n",
        "\n",
        "# read from json file\n",
        "st_time = time.time()\n",
        "df = spark.read.json(FILE_ACCESS)\n",
        "end_time = time.time()\n",
        "l.info(\"file read complete\")\n",
        "l.info(f\"file details - rows: {df.count()}, columns: {len(df.columns)}\")\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjRpUBYTZ1jy"
      },
      "outputs": [],
      "source": [
        "l.info(\"time taken to load file = \"+ str(end_time-st_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a filter to extract business located in USA only"
      ],
      "metadata": {
        "id": "JbFKe_8aWp74"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVGA8JopE419"
      },
      "outputs": [],
      "source": [
        "US_STATES_LIST = ['AL','AK','AZ','AR','AS','CA','CO','CT','DE','DC','EM','FL', 'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MH', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'VI', 'WA', 'WV', 'WI', 'WY']\n",
        "df_r_usa = df.filter(df.categories.contains(\"Restaurants\")).filter(df.state.isin(US_STATES_LIST))\n",
        "df_r_usa.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a filter to extract restaurants from all businesses"
      ],
      "metadata": {
        "id": "e__THY5bW8sh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4ktesLLGU5s"
      },
      "outputs": [],
      "source": [
        "df_restaurants = df_r_usa.select(df_r_usa.business_id,df_r_usa.name,df_r_usa.address,df_r_usa.postal_code,df_r_usa.state,df_r_usa.city,df_r_usa.latitude,df_r_usa.longitude,df_r_usa.stars,df_r_usa.review_count,df_r_usa.categories)\n",
        "df_restaurants.printSchema()\n",
        "df_restaurants.count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1sXqO4KTl0D",
        "outputId": "486d18b4-7953-4af2-a194-7ee8fadb2ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43256\n"
          ]
        }
      ],
      "source": [
        "restaurant_business_ids = df_restaurants.select('business_id').rdd.flatMap(lambda x: x).collect()\n",
        "print(len(restaurant_business_ids))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_restaurants.groupBy(\"state\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no9Cqxo_p-vl",
        "outputId": "22fdd6c1-2b1b-49a8-acde-0bb151ee0b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|state|count|\n",
            "+-----+-----+\n",
            "|   OR| 7391|\n",
            "|   WA|  773|\n",
            "|   OH| 4377|\n",
            "|   TX| 5444|\n",
            "|   GA| 6140|\n",
            "|   MA|10550|\n",
            "|   KS|    1|\n",
            "|   CO|  865|\n",
            "|   FL| 7710|\n",
            "|   MN|    1|\n",
            "|   VA|    1|\n",
            "|   WY|    1|\n",
            "|   KY|    1|\n",
            "|   NH|    1|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLwvbczZO65j"
      },
      "source": [
        "#Yelp - Review json file that contains details of reviews collected for each business"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read from hdfs/local data file"
      ],
      "metadata": {
        "id": "40m5Uoq9Xb4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCVVtnqnO9tP",
        "outputId": "678fab5e-3add-4fe7-84e3-7f2db6c078f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO | 2021-12-10 10:41:40,319 | Accesing file: file:///content/data/MyDrive/bdm-data/Yelp/yelp_academic_dataset_review.json\n",
            "INFO | 2021-12-10 10:43:53,156 | file read complete\n"
          ]
        }
      ],
      "source": [
        "HDFS_PATH= \"/content/data/MyDrive/bdm-data/Yelp/yelp_academic_dataset_review.json\"\n",
        "# if not hdfs, disregard node variable\n",
        "if \"hdfs\" not in FILESYSTEM:\n",
        "    HDFS_NODE = \"\"\n",
        "\n",
        "FILE_ACCESS = f\"{FILESYSTEM}{HDFS_NODE}{HDFS_PATH}\"\n",
        "\n",
        "l.info(f\"Accesing file: {FILE_ACCESS}\")\n",
        "\n",
        "# read from json file\n",
        "df_review = spark.read.json(FILE_ACCESS)\n",
        "l.info(\"file read complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDspORoRbaTv"
      },
      "outputs": [],
      "source": [
        "df_review.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a filter to extract reviews for resturants only"
      ],
      "metadata": {
        "id": "xdjKwJTjXeBs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZDPVpefUNif"
      },
      "outputs": [],
      "source": [
        "df_restaurants_review = df_review.filter(df_review.business_id.isin(restaurant_business_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting to Pandas Dataframe, as Spark dataframe had performance issues"
      ],
      "metadata": {
        "id": "6NFbszIgXn1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pd = df_restaurants_review.select('*').toPandas()"
      ],
      "metadata": {
        "id": "Ry0PvBbM8r5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregating all reviews for a business to a csv with business id as filename and which will also contain tweets scrapped for the particular resturant"
      ],
      "metadata": {
        "id": "N7cnHLhxXyl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_itr = df_restaurants_review.rdd.toLocalIterator()\n",
        "count = 0\n",
        "for row in data_itr:\n",
        "      f = open(f'/content/data/MyDrive/bdm-data/review-text/{row[\"business_id\"]}.txt', 'a+')\n",
        "      f.write(row[\"text\"])\n",
        "      count+=1\n",
        "      f.close()"
      ],
      "metadata": {
        "id": "t7GuJuG_xak2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0Vx0gohdTnS"
      },
      "source": [
        "#Yelp - Checkin data collected for businesses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read from hdfs/local data file"
      ],
      "metadata": {
        "id": "od6h2p7XYax6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grEwB6ZUdX2o",
        "outputId": "42f8357a-6896-4305-a924-0a3dd5a69867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO | 2021-12-10 01:17:52,184 | Accesing file: file:///Users/varunsapre/OneDrive/UCR-Fall21/Courses/CS226-BigData/project/walte-big-data/data/yelp_academic_dataset_checkin.json\n",
            "INFO | 2021-12-10 01:17:52,896 | file read complete\n",
            "INFO | 2021-12-10 01:17:53,327 | file details - rows: 138876, columns: 2\n"
          ]
        }
      ],
      "source": [
        "HDFS_PATH= \"/content/data/MyDrive/bdm-data/Yelp/yelp_academic_dataset_checkin.json\"\n",
        "# if not hdfs, disregard node variable\n",
        "if \"hdfs\" not in FILESYSTEM:\n",
        "    HDFS_NODE = \"\"\n",
        "\n",
        "FILE_ACCESS = f\"{FILESYSTEM}{HDFS_NODE}{HDFS_PATH}\"\n",
        "\n",
        "l.info(f\"Accesing file: {FILE_ACCESS}\")\n",
        "\n",
        "# read from json file\n",
        "df_checkin = spark.read.json(FILE_ACCESS)\n",
        "l.info(\"file read complete\")\n",
        "l.info(f\"file details - rows: {df_checkin.count()}, columns: {len(df_checkin.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a filter to extract checkin data for only resturants"
      ],
      "metadata": {
        "id": "4oXYeDNRYeLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4g4yYfCeL5e"
      },
      "outputs": [],
      "source": [
        "df_restaurants_checkin = df_checkin.filter(df_checkin.business_id.isin(restaurant_business_ids))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb = df_restaurants_checkin.rdd.flatMap(lambda x: x).collect()\n"
      ],
      "metadata": {
        "id": "4_dgwc7_jaU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_c_itr = df_restaurants_checkin.rdd.toLocalIterator()"
      ],
      "metadata": {
        "id": "6fVZmAt_wvKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Analysis"
      ],
      "metadata": {
        "id": "QI2_L_exYyKx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJGwYJHs5WYL"
      },
      "source": [
        "#Twint Code Space"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download twint and install. Twint can bypass the tweet limit in place for developer account"
      ],
      "metadata": {
        "id": "ARV6ABk9Y4K8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXqgl_tZ9E0U"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/twint/\n",
        "!git clone --depth=1 https://github.com/twintproject/twint.git\n",
        "!rm /content/twint/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOlHKwwp9Emx",
        "outputId": "505c5577-304c-4a96-e1d4-4351c6b89a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/twint/requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/twint/requirements.txt\n",
        "aiohttp==3.7.0\n",
        "aiodns\n",
        "beautifulsoup4\n",
        "cchardet\n",
        "dataclasses\n",
        "elasticsearch\n",
        "pysocks\n",
        "pandas>=0.23.0\n",
        "aiohttp_socks<=0.4.1\n",
        "schedule\n",
        "geopy\n",
        "fake-useragent\n",
        "googletransx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKCwV8-_5ZSa"
      },
      "outputs": [],
      "source": [
        "!cd /content/twint && pip3 install . -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggBPBoNMAO1H"
      },
      "source": [
        "#Run twint commands here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQqZsv0xM9AP"
      },
      "source": [
        "**EXPERIMENTAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfLLNqsGNKW8"
      },
      "outputs": [],
      "source": [
        "a_1=df_restaurants.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uau0TfPKSTIZ"
      },
      "outputs": [],
      "source": [
        "s = f\"{a_1[0]['latitude']},{a_1[0]['longitude']},50km,{a_1[0]['name']}\"\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create csv files containing scrapped tweets for each business, the reviews and the tweets scrapped are stored in the same place."
      ],
      "metadata": {
        "id": "p2h5P6cqZTLY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quWDlN27jlBO"
      },
      "outputs": [],
      "source": [
        "import twint\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "c = twint.Config()\n",
        "\n",
        "for i in range(5):\n",
        "  c.Search = f\"{a_1[i]['name']}\"\n",
        "  c.Near= f\"{a_1[i]['city']}\"\n",
        "  c.Limit = 100\n",
        "  c.Pandas = True\n",
        "  c.Hide_output = True\n",
        "  twint.run.Search(c)\n",
        "\n",
        "  Tweets_df = twint.storage.panda.Tweets_df\n",
        "  try:\n",
        "    temp_df = Tweets_df[['id', 'tweet']]\n",
        "  except KeyError as e:\n",
        "    print(f\"'{c.Search}' error - {e}\")\n",
        "\n",
        "  for row in temp_df.iterrows():\n",
        "    f = open(f\"/content/data/MyDrive/bdm-data/review-text/{a_1[i]['business_id']}.txt\", 'a+')\n",
        "    f.write(row[\"tweet\"])    \n",
        "    f.close()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ranking Algorithm"
      ],
      "metadata": {
        "id": "cLs9AWZeStd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read from hdfs/local data file"
      ],
      "metadata": {
        "id": "zOmA7zwAaoBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HDFS_PATH= \"/content/data/MyDrive/bdm-data/Yelp/restaurants_with_checkin_senti.csv\"\n",
        "# if not hdfs, disregard node variable\n",
        "if \"hdfs\" not in FILESYSTEM:\n",
        "    HDFS_NODE = \"\"\n",
        "\n",
        "FILE_ACCESS = f\"{FILESYSTEM}{HDFS_NODE}{HDFS_PATH}\"\n",
        "\n",
        "l.info(f\"Accesing file: {FILE_ACCESS}\")\n",
        "\n",
        "# read from json file\n",
        "df_rankingDataFrame=spark.read.format(\"csv\").option(\"header\",\"true\").load(FILE_ACCESS)\n",
        "l.info(\"file read complete\")\n",
        "l.info(f\"file details - rows: {df_rankingDataFrame.count()}, columns: {len(df_rankingDataFrame.columns)}\")\n",
        "df_rankingDataFrame.printSchema()"
      ],
      "metadata": {
        "id": "H3HeWH6KTD2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a filter to extract rows for major 8 cities that was extracted from table by groupBy and sorting by number of businesses."
      ],
      "metadata": {
        "id": "2QzSTrLabPKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cities = [\"Portland\", \"Austin\", \"Atlanta\", \"Orlando\", \"Boston\", \"Columbus\", \"Kissimmee\", \"Cambridge\", \"Boulder\"]"
      ],
      "metadata": {
        "id": "5NNJoEhm7jOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute values for the dataframe"
      ],
      "metadata": {
        "id": "SM_k0y2Ja1C1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "#intialize all variables\n",
        "sumNormalizationCheckinSum = maxNormalizationCheckinMax = minMaxScalingCheckinMin = sumNormalizationReviewCountSum = maxNormalizationReviewCountMax = minMaxScalngReviewCountMin = 0\n",
        "count = 0\n",
        "\n",
        "#compute all the value of variables, to calculate their respective rank\n",
        "for row in df_rankingDataFrame.rdd.toLocalIterator():\n",
        "  if (isinstance(row[\"checkin\"], str)and isinstance(row[\"review_count\"], str)):\n",
        "    maxNormalizationCheckinMax = maxNormalizationCheckinMax if (maxNormalizationCheckinMax > int(row[\"checkin\"])) else int(row[\"checkin\"])\n",
        "    #minMaxScalingCheckinMin = maxNormalizationCheckinMax\n",
        "    minMaxScalingCheckinMin = minMaxScalingCheckinMin if (minMaxScalingCheckinMin < int(row[\"checkin\"])) else int(row[\"checkin\"])\n",
        "    sumNormalizationCheckinSum = sumNormalizationCheckinSum + int(row[\"checkin\"])\n",
        "\n",
        "    maxNormalizationReviewCountMax = maxNormalizationReviewCountMax if (maxNormalizationReviewCountMax > int(row[\"review_count\"])) else int(row[\"review_count\"])\n",
        "    #minMaxScalngReviewCountMin = maxNormalizationReviewCountMax\n",
        "    minMaxScalngReviewCountMin = minMaxScalngReviewCountMin if (minMaxScalngReviewCountMin < int(row[\"review_count\"])) else int(row[\"review_count\"])\n",
        "    sumNormalizationReviewCountSum = sumNormalizationReviewCountSum + int(row[\"review_count\"])\n",
        "    count+=1\n",
        "\n",
        "print(minMaxScalingCheckinMin)\n",
        "print(sumNormalizationCheckinSum)\n",
        "print(maxNormalizationCheckinMax)\n",
        "print(sumNormalizationReviewCountSum)\n",
        "print(maxNormalizationReviewCountMax)\n",
        "print(minMaxScalngReviewCountMin)\n",
        "print(count)"
      ],
      "metadata": {
        "id": "URLiO2aTTMb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the final score for each row for different normalization used."
      ],
      "metadata": {
        "id": "Cz5Bn4tVa5U8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "import math\n",
        "\n",
        "#computing and assigning ranking score to each row\n",
        "for row in df_rankingDataFrame.rdd.toLocalIterator():\n",
        "  if (isinstance(row[\"checkin\"], str) and isinstance(row[\"review_count\"], str)):\n",
        " \n",
        "    maxNormalizationCheckinRank = int(row[\"checkin\"]) / maxNormalizationCheckinMax\n",
        "    sumNormalizationCheckinRank = int(row[\"checkin\"]) / sumNormalizationCheckinSum\n",
        "    minMaxScalingCheckinRank = (int(row[\"checkin\"]) - minMaxScalingCheckinMin) / maxNormalizationCheckinMax - minMaxScalingCheckinMin\n",
        "    \n",
        "    #Uncomment following statements to add individual scores\n",
        "    \"\"\"\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMaxNormalizationCheckin\", when( col(\"business_id\") == row[\"business_id\"], maxNormalizationCheckinRank))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankSumNormaizationCheckin\", when( col(\"business_id\") == row[\"business_id\"], sumNormalizationCheckinRank))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMinMaxScalinCheckin\", when( col(\"business_id\") == row[\"business_id\"], minMaxScalingCheckinRank))\n",
        "    \"\"\"\n",
        "    maxNormalizationReviewCountRank = int(row[\"checkin\"]) / maxNormalizationReviewCountMax\n",
        "    sumNormalizationReviewCountRank = int(row[\"checkin\"]) / sumNormalizationReviewCountSum\n",
        "    minMaxScalngReviewCountRank = (int(row[\"checkin\"]) - minMaxScalngReviewCountMin) / maxNormalizationReviewCountMax - minMaxScalngReviewCountMin\n",
        "\n",
        "    #Uncomment following statements to add individual scores\n",
        "    \"\"\"\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMaxNormalizationReviewCount\", when( col(\"business_id\") == row[\"business_id\"], maxNormalizationReviewCountRank))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankSumNormaizationReviewCount\", when( col(\"business_id\") == row[\"business_id\"], sumNormalizationReviewCountRank))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMinMaxScalinReviewCount\", when( col(\"business_id\") == row[\"business_id\"], minMaxScalngReviewCountRank))\n",
        "    \"\"\"\n",
        "\n",
        "    finalMaxNormalizationScore = (math.log(maxNormalizationReviewCountRank) if (maxNormalizationReviewCountRank > 0) else 0)  + (math.log(maxNormalizationCheckinRank) if (maxNormalizationCheckinRank > 0) else 0) \n",
        "    finalSumNormalizationScore = (math.log(sumNormalizationReviewCountRank) if (sumNormalizationReviewCountRank > 0) else 0)  + (math.log(sumNormalizationCheckinRank) if (sumNormalizationCheckinRank > 0) else 0) \n",
        "    finalMinMaxScalingScore = (math.log(minMaxScalngReviewCountRank) if (minMaxScalngReviewCountRank > 0) else 0)  + (math.log(minMaxScalingCheckinRank) if (minMaxScalingCheckinRank > 0) else 0) \n",
        "    \n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMaxNormalizationScore\", when( col(\"business_id\") == row[\"business_id\"], finalMaxNormalizationScore))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankSumNormaizationScore\", when( col(\"business_id\") == row[\"business_id\"], finalSumNormalizationScore))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMinMaxScalinReviewCountScore\", when( col(\"business_id\") == row[\"business_id\"], finalMinMaxScalingScore))\n",
        "    \n",
        "    count += 1\n",
        "    print(count)\n",
        "\n",
        "  else:\n",
        "    \n",
        "    #Uncomment following statements to add individual scores\n",
        "    \"\"\"\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMaxNormalizationCheckin\", when( col(\"business_id\") == row[\"business_id\"], 0))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankSumNormaizationCheckin\", when( col(\"business_id\") == row[\"business_id\"], 0))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMinMaxScalinCheckin\", when( col(\"business_id\") == row[\"business_id\"], 0))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMaxNormalizationReviewCount\", when( col(\"business_id\") == row[\"business_id\"], 0))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankSumNormaizationReviewCount\", when( col(\"business_id\") == row[\"business_id\"], 0))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMinMaxScalinReviewCount\", when( col(\"business_id\") == row[\"business_id\"], 0))\n",
        "    \"\"\"\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMaxNormalizationScore\", when( col(\"business_id\") == row[\"business_id\"], 0))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankSumNormaizationScore\", when( col(\"business_id\") == row[\"business_id\"], 0))\n",
        "    df_rankingDataFrame = df_rankingDataFrame.withColumn(\"rankMinMaxScalinReviewCountScore\", when( col(\"business_id\") == row[\"business_id\"], 0))\n",
        "    "
      ],
      "metadata": {
        "id": "3rkk-AfjoZAK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hLwvbczZO65j",
        "A0Vx0gohdTnS",
        "Rd1doThfkm2x",
        "xJGwYJHs5WYL",
        "ggBPBoNMAO1H"
      ],
      "name": "Copy of BDM-What_America_Likes_To_Eat.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}